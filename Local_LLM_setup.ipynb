{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyTt0-ITFcvh"
      },
      "source": [
        "# 1. Ollama 설치\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjXmAeuKF4KO"
      },
      "outputs": [],
      "source": [
        "https://github.com/ollama/ollama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktIQIl9_JVfs"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sF-YnO-xF5sy"
      },
      "source": [
        "\n",
        "# 2. Terminal 에서 Llama3 실행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7myDQghF74N"
      },
      "outputs": [],
      "source": [
        "ollama run llama3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaItSqznJUlz"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQ1GvqqJF9Ue"
      },
      "source": [
        "# 3. API 를 통해 Ollama Llama3 모델에 프롬프트 입력하고 결과 받기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKRH5y0vFzFt"
      },
      "outputs": [],
      "source": [
        "POST http://localhost:11434/api/chat\n",
        "{\n",
        "  \"model\": \"llama3\",\n",
        "  \"stream\": false,\n",
        "  \"messages\": [\n",
        "    { \"role\": \"system\", \"content\": \"Convert the following natural language requests into SQL queries:\\n\"},\n",
        "    { \"role\": \"user\", \"content\": \"why is the sky blue?\" }\n",
        "  ]\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vqqv0OpbJS0_"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQgBOUMAGH6V"
      },
      "source": [
        "# 4. Model phi3로 변경해서 2, 3 해보기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ESZZurnGEjH"
      },
      "outputs": [],
      "source": [
        "ollama run phi3\n",
        "\n",
        "POST http://localhost:11434/api/chat\n",
        "{\n",
        "  \"model\": \"phi3\",\n",
        "  \"stream\": false,\n",
        "  \"messages\": [\n",
        "    { \"role\": \"system\", \"content\": \"Convert the following natural language requests into SQL queries:\\n\"},\n",
        "    { \"role\": \"user\", \"content\": \"why is the sky blue?\" }\n",
        "  ]\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fDFZ69sJei2"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRyd-90wGiEK"
      },
      "source": [
        "# 5. 사전에 작성한 2개의 역할 중 무엇이 사용자 입력 프롬프트에 대해 더 잘 답할 수 있을지 답변하는 프롬프트 작성하기\n",
        "(질문에 대한 답변을 하는 것이 아니라 2개의 역할 중 어떤 역할이 잘 답할지를 답변하도록 하기)\n",
        "# 6. 5에서 작성한 프롬프트와 역할을 이용해, 사용자 입력에 적합한 역할로써 사용자 입력에 답하는 프롬프트 및 코드 작성하기\n",
        "# 7. 오늘 배운 프롬프트 엔지니어링을 활용해 5번의 각 역할이 더 대답을 잘할 수 있도록 프롬프트 개선하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgi11GNJGEcn",
        "outputId": "a4c044ad-a9c9-49d6-83ef-f3b27310ff61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.37.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n"
          ]
        }
      ],
      "source": [
        "pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "ZsTOkOQ_JCFR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=\"sk-\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UiJ_ClBvh4A",
        "outputId": "172b9b3e-0ebb-4e42-b608-abbcbf7636d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Role 2 is more suitable for answering the user's question. The question pertains to a historical event, specifically the conclusion of World War I, which falls directly under the expertise of Role 2, the history teacher. Role 1, being a math teacher, would not have the relevant knowledge to provide an informed response about historical events.\n"
          ]
        }
      ],
      "source": [
        "system_message = f\"\"\"\n",
        "**System Instruction:** You are an AI evaluator designed to determine which of two given roles (Role 1 or Role 2) is better suited to answer a user's question. You must carefully consider the capabilities, knowledge, and context of each role to make an accurate assessment.\n",
        "\n",
        "**Input:**\n",
        "- Role 1: [Description of Role 1's expertise, knowledge, and capabilities]\n",
        "- Role 2: [Description of Role 2's expertise, knowledge, and capabilities]\n",
        "- User's Question: [The user's specific question]\n",
        "\n",
        "**Output:**\n",
        "Evaluate the user's question and provide a concise decision on which role (Role 1 or Role 2) is more appropriate to answer the question. Justify your decision with a brief explanation based on the provided descriptions of each role's expertise and capabilities.\n",
        "\n",
        "**Example:**\n",
        "Role 1: \"Expert in data science and machine learning. Skilled in statistical analysis, data visualization, and algorithm development.\"\n",
        "Role 2: \"Expert in software engineering and system architecture. Skilled in coding, system design, and performance optimization.\"\n",
        "User's Question: \"How can I optimize a machine learning algorithm for better performance?\"\n",
        "\n",
        "**Evaluation:**\n",
        "Role 1 is more suitable for answering the user's question. This is because the question pertains to optimizing a machine learning algorithm, which falls directly under the expertise of Role 1 in data science and machine learning. Role 1's skills in statistical analysis and algorithm development make it the best fit to provide an informed and accurate response.\n",
        "\n",
        "**Instructions for AI:**\n",
        "1. Analyze the expertise and capabilities of Role 1 and Role 2.\n",
        "2. Read and understand the user's question.\n",
        "3. Compare the question's requirements with the expertise of each role.\n",
        "4. Decide which role is more appropriate for answering the question.\n",
        "5. Provide a clear decision and a brief justification for your choice.\n",
        "\n",
        "**Constraints:**\n",
        "- Ensure that the decision is based solely on the provided descriptions of Role 1 and Role 2.\n",
        "- Justifications should be concise, focusing on the relevance of each role's expertise to the user's question.\n",
        "\"\"\"\n",
        "\n",
        "user_message = f\"\"\"\n",
        "Role 1: math teather\n",
        "Role 2: history teather\n",
        "User's Question: How did World War I end?\n",
        "\"\"\"\n",
        "\n",
        "result = get_chatgpt_response(system_message = system_message, user_message = user_message, template=0.2)\n",
        "print(result)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
