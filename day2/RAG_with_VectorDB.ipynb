{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# RAG + Chain"
   ],
   "metadata": {
    "id": "ENWQqUQMh5KJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain langchain-openai langchain-openai langchain_chroma langchain-text-splitters langchain_community langchainhub nest_asyncio langchain-text-splitters langchain-openai sentence-transformers"
   ],
   "metadata": {
    "id": "16ynpUpWh7AP"
   },
   "execution_count": 106,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ],
   "metadata": {
    "id": "D0Abuadih7Cs"
   },
   "execution_count": 107,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tutorial"
   ],
   "metadata": {
    "id": "0yjOlxpIkhPm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    ").to_messages()\n",
    "\n",
    "example_messages"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ruilm9cWh7FC",
    "outputId": "ab2cc6ef-3f67-4331-81c3-aacd8b8c023b"
   },
   "execution_count": 108,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: filler question \\nContext: filler context \\nAnswer:\")]"
      ]
     },
     "metadata": {},
     "execution_count": 108
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load, chunk and index the contents of the blog.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "H0MoLB_bh7HX",
    "outputId": "8d0af411-f33b-4f57-f2ca-24661a6fc0d4"
   },
   "execution_count": 109,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Task Decomposition refers to the process of breaking down complex tasks into smaller, more manageable steps. This is often achieved through a technique called Chain of Thought (CoT), where models are prompted to \"think step by step.\" This approach not only enhances performance on difficult tasks but also provides insight into the model\\'s reasoning process.'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 109
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1. 3개의 블로그 포스팅 본문을 Load: WebBaseLoader 활용**\n",
    "https://python.langchain.com/v0.2/docs/integrations/document_loaders/web_base/"
   ],
   "metadata": {
    "id": "tmz3In9a64o_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "loaded_documents = WebBaseLoader(urls).aload()\n",
    "contents = [x.page_content for x in loaded_documents]\n",
    "content = \"\\n\".join(contents)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tJ2Vvt2n6EWX",
    "outputId": "56442a38-423c-44e0-d756-97863059c75e"
   },
   "execution_count": 110,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Fetching pages: 100%|##########| 3/3 [00:00<00:00, 16.68it/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2. 불러온 본문을 Split (Chunking) : recursive text splitter 활용**\n",
    "\n",
    "https://python.langchain.com/v0.2/docs/how_to/recursive_text_splitter/"
   ],
   "metadata": {
    "id": "p5m9Lvvp7LRz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "cleaned_content = re.sub(r'\\n+', '\\n', content)"
   ],
   "metadata": {
    "id": "FqqZ2pJfSquT"
   },
   "execution_count": 111,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "chunked_contents = text_splitter.split_text(cleaned_content)"
   ],
   "metadata": {
    "id": "Frn-C-BjK_ol"
   },
   "execution_count": 112,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**3. Chunks 를 임베딩하여 Vector store 저장: openai, chroma 사용**\n",
    "\n",
    "embedding model 은 \"text-embedding-3-small\" 사용\n",
    "\n",
    "embedding: https://python.langchain.com/v0.2/docs/integrations/text_embedding/openai/\n",
    "\n",
    "vetor store: https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/"
   ],
   "metadata": {
    "id": "0NRSXQzp7Tb7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import chromadb\n",
    "\n",
    "\n",
    "model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "new_client = chromadb.EphemeralClient()\n",
    "vectorstore = Chroma.from_texts(\n",
    "    chunked_contents,\n",
    "    model,\n",
    "    client=new_client,\n",
    "    collection_name=\"openai_collection\"\n",
    ")"
   ],
   "metadata": {
    "id": "XpjqTXzb5_-S"
   },
   "execution_count": 113,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "query = \"Barack Obama\"\n",
    "docs = vectorstore.similarity_search(query)\n",
    "print(docs)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fAl9F-L0fL7V",
    "outputId": "50c0b4e0-329b-4fb0-fa64-3a7c92cf7a37"
   },
   "execution_count": 114,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Document(page_content='with \"Barack Obama\" but leading to toxic output. Given an auditing objective $\\\\phi: \\\\mathcal{X}'), Document(page_content='with \"Barack Obama\" but leading to toxic output. Given an auditing objective $\\\\phi: \\\\mathcal{X}'), Document(page_content='with \"Barack Obama\" but leading to toxic output. Given an auditing objective $\\\\phi: \\\\mathcal{X}'), Document(page_content='\\\\mathbf{y})$ that match certain behavior pattern; such as non-toxic input starting with \"Barack')]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**4. User query = ‘agent memory’ 를 받아 관련된 chunks를 retrieve**\n",
    "\n",
    "https://python.langchain.com/v0.2/docs/how_to/vectorstore_retriever/\n",
    "\n",
    "https://api.python.langchain.com/en/latest/vectorstores/langchain_core.vectorstores.VectorStore.html#langchain_core.vectorstores.VectorStore.as_retriever"
   ],
   "metadata": {
    "id": "TedWnhnX7Xhz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
    "\n",
    "docs = retriever.invoke(\"what did the president say about ketanji brown jackson?\")"
   ],
   "metadata": {
    "id": "otieqTlU7Zw8"
   },
   "execution_count": 120,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**5. User query와 retrieved chunk 에 대해 relevance 가 있는지를 평가하는 시스템 프롬프트 작성: retrieval 퀄리티를 LLM 이 스스로 평가하도록 하고, 관련이 있으면 {‘relevance’: ‘yes’} 관련이 없으면 {‘relevance’: ‘no’} 라고 출력하도록 함. ( JsonOutputParser() 를 활용 )**\n",
    "\n",
    "RAG 용 프롬프트 작성을 위한 Prompt Hub 활용\n",
    "\n",
    "https://smith.langchain.com/hub/rlm/rag-prompt"
   ],
   "metadata": {
    "id": "xJ5tEr427aE0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def checkRelevance(user_query, content):\n",
    "    parser = JsonOutputParser()\n",
    "    prompt = PromptTemplate(\n",
    "        temperature=\"0.2\",\n",
    "        template=\"Please answer yes or no if {user_query} and {content} are related. Answer template is given as a string in json form, such as 'relevance': 'yes' or 'relevance': 'no'. The result must be a string wrapped in a string.\",\n",
    "        input_variables=[\"user_query\", \"content\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "    chain = prompt | llm | parser\n",
    "    return chain.invoke({\"user_query\": user_query, \"content\": content})"
   ],
   "metadata": {
    "id": "zAAA0X2xtPOz"
   },
   "execution_count": 148,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "checkRelevance(\"what did the president say about ketanji brown jackson?\", docs[0].page_content)['relevance']"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "uawMPXj_uf-Z",
    "outputId": "4e4d4ef2-ecf5-4a6c-ad4a-17f2c33334e3"
   },
   "execution_count": 149,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'no'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 149
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain import hub\n",
    "\n",
    "rlm_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "rlm_prompt"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CR3AAhWU7lux",
    "outputId": "5d034c1a-677f-4980-aa8b-37c11da865b6"
   },
   "execution_count": 141,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"
      ]
     },
     "metadata": {},
     "execution_count": 141
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "output JSON formatting\n",
    "\n",
    "https://python.langchain.com/v0.2/docs/how_to/output_parser_json/#without-pydantic"
   ],
   "metadata": {
    "id": "ra4PLuBz7lTu"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. 5 에서 모든 docs에 대해 ‘no’ 라면 디버깅 (Splitter, Chunk size, overlap, embedding model, vector store, retrieval 평가 시스템 프롬프트 등)"
   ],
   "metadata": {
    "id": "Wpe3aUWH73QQ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "7. 5에서 ‘yes’ 라면 질문과 명확히 관련 없는 docs 나 질문 (예: ‘I like an apple’)에 대해서는 ‘no’ 라고 나오는지 테스트 프롬프트 및 평가 코드 작성. 이 때는 관련 없다는 답변 작성"
   ],
   "metadata": {
    "id": "FFgcrR3g75xr"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "8. ‘yes’ 이고 7의 평가에서도 문제가 없다면, 4의 retrieved chunk 를 가지고 답변 작성\n",
    "prompt | llm | parser 코드 작성"
   ],
   "metadata": {
    "id": "jeXqxkJ375vf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for chunk in rag_chain.stream(\"What is Task Decomposition?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ],
   "metadata": {
    "id": "dflv8MB57-Dz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "9. 생성된 답안에 Hallucination 이 있는지 평가하는 시스템 프롬프트 작성. LLM이 스스로 평가하도록 하고, hallucination 이 있으면 {‘hallucination’: ‘yes’} 없으면 {‘hallucination’: ‘no’} 라고 출력하도록 함"
   ],
   "metadata": {
    "id": "uk972sM875s-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "10. 9 에서 ‘yes’ 면 8 로 돌아가서 다시 생성, ‘no’ 면 답변 생성하고 유저에게 답변 생성에 사용된 출처와 함께 출력 (최대 2번까지 다시 생성)"
   ],
   "metadata": {
    "id": "YSjyzY3U75qq"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "11. https://applied-llms.org/ 링크는 llm 을 이용해 1년간 개발해 본 팀이 배운 것들을 정리한 아티클 입니다.\n",
    "\n",
    "저자의 생각을 묻는 질문들에 답하기 위해서\n",
    "\n",
    "1) Chunking & embedding & storing\n",
    "\n",
    "2) Load\n",
    "\n",
    "3) Retrieval\n",
    "\n",
    "4) Generation\n",
    "\n",
    "으로 구성된 RAG 코드를 작성하고 아래 예시 질문에 대한 답을 해보세요.\n",
    "\n",
    "예시)\n",
    "RAG 에 대한 저자의 생각은 무엇인가?\n",
    "RAG 와 fine tuning 에 대해 저자는 어떻게 비교하고 있나?\n",
    "저자가 가장 많은 부분을 할당해 설명하는 개념은 무엇인가?\n",
    "\n",
    "11 실습을 마치지 못했더라도, https://applied-llms.org/ 아티클은 꼼꼼히 정독해보시기를 추천 드립니다."
   ],
   "metadata": {
    "id": "jXUS-Lj575k-"
   }
  }
 ]
}
